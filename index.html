<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="An Introduction page to Union14M dataset and MAERec.">
  <meta name="keywords" content="STR, Union14M, MAERec">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Revisiting Scene Text Recognition: A Data Perspective</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/title.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Revisiting Scene Text Recognition: A Data Perspective</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://mountchicken.github.io/">Qing Jiang</a> ,</span>
              <span class="author-block">
                <a href="https://github.com/jpWang">Jiapeng Wang</a> ,</span>
              <span class="author-block">
                <a href="https://github.com/shannanyinxiang">Dezhi Peng</a> ,
              </span>
              <span class="author-block">
                <a href="https://github.com/lcy0604">Chongyu Liu</a> ,
              </span>
              <span class="author-block">
                <a href="http://www.dlvc-lab.net/lianwen/">Lianwen Jin</a><sup>*</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">South China University of Technology</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- arxiv Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2307.08723"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/Mountchicken/Union14M"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Demo Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/Mountchicken/MAERec-Gradio"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Demo</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://github.com/Mountchicken/Union14M#34-download"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Two Questions for Scene Text Recognition</h2>
          <div class="content has-text-justified">
            <p>
              Since the introduction of the end-to-end network CRNN, Scene Text Recognition (STR) has
              embarked on a fast-paced journey driven by deep learning. With the continuous emergence of new methods,
              the accuracy of
              the six commonly used benchmarks in the STR field has been steadily improving. As shown below, the latest
              advancements in the STR field are exhibiting a saturation trend in accuracy. The challenges posed by the
              commonly used
              benchmarks appear to have been "solved," evident in the narrow room for improvement in accuracy and the
              recent
              deceleration in performance gains by SOTA models.
            </p>
           
            <p>
            <div align=center>
              <img src="static/images/benchmark_acc.png" style="width: 60%" alt>
            </div>
            </p>
            <p>
              This phenomenon has prompted us to raise the following two questions: 1) Are the commonly used benchmark
              still
              sufficient to drive future progress in the field of STR? 2) Does the saturation of accuracy in these
              benchmark
              imply that the problem of STR has been solved?
            </p>

          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Q1: Are common benchmarks remain challenging?</h2>
          <div class="content has-text-justified">
            <p>
              we start by selecting 13 representative models, including CTC-based,
              attention-based and language model-based models.
              We evaluate their performance on the six STR benchmarks to find their joint errors. As
              depicted below, only 3.9% (298 images) of the
              total 7672 benchmark images can not be correctly
              recognized by any models. And there might be only 1.53% scope for
              accuracy improvement. Therefore, the common benchmarks give limited insight into future STR research.
            </p>
            <p>
            <div align=center>
              <img src="static/images/error.png" style="width: 60%" alt>
            </div>
            </p>
            <p>
            <div align=center>
              <img src="static/images/benchmark_error.png" style="width: 100%" alt>
            </div>
            </p>

          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Q2: Is STR solved or challenges are obscured?</h2>
          <div class="content has-text-justified">
            <p>
              While the benchmarks for STR have somehow reached their upper limits, it does not necessarily imply that the
              problem of STR
              has been completely solved. To address this question, we constructed a large-scale, real-world scene text
              recognition dataset called Union14M. Union14M is composed of 17 publicly available datasets, including 4
              million labeled
              data samples (Union14M-L) and 10 million high-quality unlabeled data samples (Union14M-U). As shown below
              Union14M encompasses diverse real-world scenes, such as curved text, multi-oriented text, artistic fonts,
              text on
              complex backgrounds, and blurred/occluded text. It can be viewed as a comprehensive mapping of text
              distributions in
              real-world scenarios, enabling us to evaluate the performance of STR models in more complex and diverse
              real-world
              scenes.
            </p>
            <p>
            <div align=center>
              <img src="static/images/union14m.png" style="width: 60%" alt>
            </div>
            </p>
            <p>
              We conducted experiments using Union14M-L on 13 STR algorithms, comparing
              their accuracy
              on both the commonly used benchmarks and Union14M-L. As shown in below, although the 13 STR
              algorithms achieved
              high performance on the commonly used STR benchmarks, they exhibited an average accuracy drop of
              20.50% on
              Union14M-L. This indicates that existing STR algorithms trained on synthetic data lack generalization and
              are far from
              meeting the demands of real-world scenarios. It also addresses the previous question: STR is far from
              being solved.
            </p>
            <p>
            <div align=center>
              <img src="static/images/performance.png" style="width: 60%" alt>
            </div>
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Remaining Challenges for STR</h2>
          <div class="content has-text-justified">
            <p>
              We summarized seven challenges for STR in real-world scenarios through error analysis on Union14M-L. Details 
              can be found in our paper
            </p>
            <p>
            <div align=center>
              <img src="static/images/errorany.png" style="width: 100%" alt>
            </div>
            </p>

            <p>
              To assess STR models in a more comprehensive range of real-world scenarios and promote future research on the seven
              challenges mentioned earlier, we have constructed a question-driven benchmark called Union14M-Benchmark. It consists of
              eight subsets, comprising a total of 409,393 images.
            </p>
            <div align=center>
              <img src="static/images/benchmark.jpg" style="width: 100%" alt>
            </div>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Benchmark Experiments</h2>
          <div class="content has-text-justified">
          <p>
            We conducted benchmark tests on the aforementioned 13 STR models using Union14M-L to provide more quantitative analysis
            conclusions. The results are presented in Table 5.
          </p>
          <p>
          <div align=center>
            <img src="static/images/experiment.png" style="width: 80%" alt>
          </div>
          </p>
          <ul>
            <li> 
              <b>Conclusion 1: Real-world data is challenging.</b> As shown in Tables 5 and 6, compared to the commonly used benchmarks,
              the average accuracy of models on Union14M-Benchmark decreased by 48.5% and 33.0% when trained on synthetic datasets and
              Union14M-L, respectively. This indicates that text images in real-world scenarios are more complex than those in the six
              commonly used benchmarks.
            </li>
            <li>
              <b>Conclusion 2: Real-world data is effective.</b> Models trained on Union14M-L can achieve an average accuracy improvement of
              3.9% on the commonly used benchmarks and a 19.6% improvement on Union14M-Benchmark. The relatively small
              performance improvement on the common benchmarks also suggests their saturation state.
            </li>
            <li>  
              <b>Conclusion 3: STR is still far from being solved.</b> When trained only on Union14M-L, we observed a maximum average
              accuracy of only 74.6% on Union14M-Benchmark. This indicates that STR is far from being solved. Although relying on
              large-scale real-world data can bring certain performance improvement, further research is still needed in the STR
              community.
            </li>
          </ul>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">MAERec: A Real-world Adapted Recognizer</h2>
          <div class="content has-text-justified">
          <p>
            To further explore the potential of addressing STR from a data perspective, we propose a scene text recognizer called
            MAERec, which is based on self-supervised pre-training using the MAE paradigm. To leverage the 10 million unlabeled
            images in Union14M-U, we pre-train the ViT backbone network in MAERec using the MAE pre-training paradigm.
            After pre-training, we initialize MAERec with the pre-trained ViT weights and fine-tune the entire
            model on Union14M-L. When using ViT-Base as the backbone network, MAERec achieves an
            average accuracy of 85.2% on Union14M-Benchmark. This result demonstrates that utilizing large-scale unlabeled data can
            significantly improve the performance of STR models in real-world scenarios and warrants further exploration.
          </p>
          <div align=center>
            <img src="static/images/maerec.png" style="width: 80%" alt>
          </div>
          </div>
        </div>
      </div>
  </section>



  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{jiang2023revisiting,
      title={Revisiting Scene Text Recognition: A Data Perspective}, 
      author={Qing Jiang and Jiapeng Wang and Dezhi Peng and Chongyu Liu and Lianwen Jin}
      booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
      year={2023}
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/pdf/2307.08723">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/Mountchicken/Union14M" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="content">
          <p>
            The template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
